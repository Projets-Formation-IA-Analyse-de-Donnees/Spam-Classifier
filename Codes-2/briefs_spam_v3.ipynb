{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDYnD_yh-skJ"
   },
   "source": [
    "# Classifieur de Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhgi21K2-nFa"
   },
   "source": [
    "lien du brief : https://simplonline.co/briefs/97a4822f-8af0-4607-86b3-83dbfdd05d5e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptif de Simplonline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contexte :\n",
    "\n",
    "Concevoir un classifieur de détection automatique de SPAM.\n",
    "\n",
    "La collection SMS Spam est un ensemble de messages SMS marqués qui ont été collectés pour la recherche sur les SMS Spam. Elle contient un ensemble de messages SMS en anglais de 5 574 messages, étiquetés selon qu'ils sont ham (légitimes) ou spam.\n",
    "Je vous encourage à vous documenter sur les caractéristiques type des spam et de développer votre stratégie de préparation des données dans ce sens.\n",
    "\n",
    "En tant que développeur IA, voici les missions :\n",
    "- Analyse du besoin\n",
    "- Construction d'un pipeline de ML\n",
    "- Prétraitement des données\n",
    "- Entrainement, fine tuning, validation et sélection d'un modèle de classification\n",
    "\n",
    "Les fichiers contiennent un message par ligne. Chaque ligne est composée de deux colonnes : v1 contient le label (ham ou spam) et v2 contient le texte brut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "liens :\n",
    "\n",
    "dataset : https://github.com/remijul/dataset/blob/master/SMSSpamCollection\n",
    "\n",
    "informations : https://archive.ics.uci.edu/dataset/228/sms+spam+collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critères de performance :\n",
    "\n",
    "- compréhension du jeux de données\n",
    "- capacité à préparer les données\n",
    "- performance des modèles de prédiction\n",
    "- capacité à apporter une solution dans le temps imparti\n",
    "- rédaction du notebook\n",
    "- qualité du synthèse du travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Livrables :\n",
    "\n",
    "* créer un/des notebook reproductible, commenté, expliqué (IMPORTANT !)\n",
    "* créer un repo git et un espace sur github/gitlab pour le projet (code refactorisé)\n",
    "* faire une présentation (slides) qui explique votre démarche et les résultats obtenus avec :\n",
    "- un document technique qui explique l'outil\n",
    "- la procédure suivie pour préparer les données et le preprocessing\n",
    "- la procédure suivie pour trouver un modèle adapté\n",
    "- le modèle d'IA sélectionné\n",
    "\n",
    "BONUS :\n",
    "* Application streamlit qui fait de la prédiction en temps réel d'un message déposé par l'utilisateur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du contexte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D'où viennent les données : Par qui ? Pour quoi ? Comment ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMS Spam Collection est un ensemble public de messages étiquetés par SMS qui ont été collectés pour la recherche sur le spam pour les téléphones portables.\n",
    "\n",
    "##### Instances = 5574\n",
    "\n",
    "##### Informations supplémentaires\n",
    "\n",
    "Ce corpus a été collecté à partir de sources de recherche gratuites ou gratuites sur Internet:\n",
    "\n",
    "Une collection de 425 messages de spam par SMS a été extraite manuellement du site Web de Grumbletext. Il s'agit d'un forum britannique dans lequel les utilisateurs de téléphones portables font des déclarations publiques sur les SMS spam, la plupart d'entre eux sans signaler le message de spam reçu. L'identification du texte des messages de spam dans les revendications est une tâche très difficile et longue, et il a consisté à numériser soigneusement des centaines de pages Web. Le site Web de Grumbletext est le suivant: http://www.grumbletext.co.uk/.\n",
    "Un sous-ensemble de 3 375 SMS choisis au hasard par jambon du NUS SMS Corpus (NSC), qui est un ensemble de données d'environ 10 000 messages légitimes collectés pour la recherche au Département de l'informatique de l'Université nationale de Singapour. Les messages proviennent en grande partie de Singapouriens et principalement d'étudiants fréquentant l'Université. Ces messages ont été recueillis auprès de volontaires qui ont été informés que leurs contributions allaient être rendues publiques. Le NUS SMS Corpus est disponible à l'adresse suivante: http://www.comp.nus.edu.sg/.rpnlpir/downloads/corpora/smsCorpus/.\n",
    "Une liste de 450 SMS de type jambon collectés sur la thèse de doctorat de Caroline Tag disponible à l'adresse http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf.\n",
    "Enfin, nous avons incorporé le SMS Spam Corpus v.0.1 Big. Il contient 1 002 messages de mja SMS et 322 messages de spam et il est disponible en public à l'adresse suivante: http://www.esp.uem.es/jmgomez/smsspamcorpus/. Ce corpus a été utilisé dans les recherches universitaires suivantes:\n",
    "\n",
    "1 G-3mez Hidalgo, J.M., Cajigas Bringas, G., Puertas Sanz, E., Carrero Garcia, F. Filtration par SMS basée sur le contenu. Actes du Colloque 2006 de l'ACM sur l'ingénierie des documents (ACM DOCENG'06), Amsterdam (Pays-Bas), 10-13, 2006.\n",
    "\n",
    "Cormack, G. V., G-3mez Hidalgo, J. M., et Puertas Sonz, E. Ingénierie technique pour filtrage de spam mobile (SMS).  Actes de la trentième Conférence internationale annuelle de la CMA sur la recherche et le développement dans la recherche et le développement dans le domaine de la recherche et de l'information (ACM SIGIR'07), New York, NY, 871-872, 2007.\n",
    "\n",
    "3 Cormack, G. V., G-3mez Hidalgo, J. M., et Puertas Sonz, E. Filtration de spam pour les messages courts. Actes de la seizième Conférence de l'ACM sur la gestion de l'information et des connaissances (ACM CIKM'07). Lisbonne, Portugal, 313-320, 2007.\n",
    "\n",
    "##### Des valeurs manquantes ont-elles été des valeurs?\n",
    "\n",
    "Non\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quoi on reconnait un Spam ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Généralement, les messages malveillants sont envoyés à destination d'un grand nombre de cibles, ils ne sont pas ou peu personnalisés.\n",
    "\n",
    "- Le message évoque un dossier, une facture, un thème qui ne vous parle pas ? Il s'agit certainement d'un courriel malveillant.\n",
    "\n",
    "(source : https://www.economie.gouv.fr/entreprises/comment-lutter-contre-spams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment faire pour reconnaitre un Spam à partir d'un texte ? (hypotèse de travail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rechercher dans le texte brut :\n",
    "- des mots clé comme : 'URGENT!', 'Quiz!', 'YOU!', 'Txt:', 'now!', 'Call ', 'Win', 'WINNER', '!!', \n",
    "- des montions à de l'argent\n",
    "- des numéros de téléphone\n",
    "- des e-mails\n",
    "- des liens\n",
    "- utilisation de mot en majuscule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, OrdinalEncoder, StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC, SVR, LinearSVC, LinearSVR, NuSVC, NuSVR, OneClassSVM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOB7I9fDarj7"
   },
   "source": [
    "## Amélioration du prétraitement et du model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refaire préproces et modelisation avec une pipeline pour être plus éfficace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "préparation du netoyage des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taitement_na_duplic (df) :\n",
    "    \"\"\"\n",
    "    entrée : un data frame\n",
    "    sortie : 2 data frame = 'principal' et 'na'\n",
    "    ---------------------------\n",
    "    \"\"\"\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    df.rename(columns={0:'classification', '0':'classification'}, inplace=True)\n",
    "    df.rename(columns={1:'sms', '1':'sms'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "préparation de l'encodage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_cle_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de caractère\n",
    "    sortie : boolean\n",
    "    ---------------------\n",
    "    j'ai une liste de mots clés\n",
    "    je crée le pattern des mots clés\n",
    "    je recherche dans la colonne 'sms' si je trouve le pattern  \n",
    "    \"\"\"\n",
    "    mot_cles = ['URGENT!', 'Quiz!', 'YOU!', 'Txt:', 'now!', 'Call ', 'Win', 'WINNER', '!!', 'For sale', 'FREE!', 'PRIVATE!', 'Account', 'Latest News!']\n",
    "    pattern = re.compile(r\"(?=(\"+'|'.join(mot_cles)+r\"))\", re.IGNORECASE)\n",
    "    match = re.findall(pattern, sms)\n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argent_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de caractère\n",
    "    sortie : boolean\n",
    "    ---------------------\n",
    "    j'ai une liste de mots clés\n",
    "    je crée le pattern des mots clés\n",
    "    je recherche dans la colonne 'sms' si je trouve le pattern  \n",
    "    \"\"\"\n",
    "    mot_cles = ['£', '€', '\\$']\n",
    "    pattern = re.compile(r\"(?=(\"+'|'.join(mot_cles)+r\"))\", re.IGNORECASE)\n",
    "    match = re.findall(pattern, sms)\n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telephone_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de carractère\n",
    "    sortie : boolean\n",
    "    ---------------------\n",
    "    crée le pattern des numero de tel\n",
    "    recherche dans une chaine de caractère si je trouve le pattern    \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"(\\+\\d{1,3})?\\s?\\(?\\d{1,4}\\)?[\\s.-]?\\d{1,4}[\\s.-]?\\d{1,4}\")\n",
    "    match = re.search(pattern, sms)\n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de caractère\n",
    "    sortie : boolean\n",
    "    ---------------------\n",
    "    je crée le pattern des e-mails\n",
    "    je recherche dans la colonne 'sms' si je trouve le pattern    \n",
    "    \"\"\"\n",
    "    pattern = r\"([A-Za-z0-9]+[.-_])*[A-Za-z0-9]+@[A-Za-z0-9-]+(\\.[A-Z|a-z]{2,})+\"\n",
    "    match = re.findall(pattern, sms)\n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lien_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de caractère\n",
    "    sortie : boolean\n",
    "    ---------------------\n",
    "    j'ai une liste de mots clés\n",
    "    je crée le pattern des mots clés\n",
    "    je recherche dans la colonne 'sms' si je trouve le pattern  \n",
    "    \"\"\"\n",
    "    mot_cles = ['http', 'https', 'www.', 'click here']\n",
    "    pattern = re.compile(r\"(?=(\"+'|'.join(mot_cles)+r\"))\", re.IGNORECASE)\n",
    "    match = re.findall(pattern, sms)\n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_maj_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de caractère\n",
    "    sortie : boolean\n",
    "    ---------------------\n",
    "    je crée le pattern des majuscules\n",
    "    je recherche dans la colonne 'sms' si je trouve le pattern  \n",
    "    \"\"\"\n",
    "    pattern = \"[A-Z]{3}\"\n",
    "    match = re.findall(pattern, sms)\n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de caractère\n",
    "    sortie : int\n",
    "    ---------------------\n",
    "    je mesure la taille de chaque ligne de la colonne 'sms'\n",
    "    \"\"\"\n",
    "    return int(len(sms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_mot_posible (sms) :\n",
    "    \"\"\"\n",
    "    entrée : chaine de caractère\n",
    "    sortie : int\n",
    "    ---------------------\n",
    "    je mesure le nombre de mots de chaque ligne de la colonne 'sms'\n",
    "    \"\"\"\n",
    "    list_of_words = sms.split()\n",
    "    return int(len(list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisation_df (df) :\n",
    "    \"\"\"\n",
    "    entrée : un data frame\n",
    "    sortie : un data frame\n",
    "    ---------------------------\n",
    "    je crée la colonne 'mot_cles' grâce à la fonction 'mot_cle_posible'\n",
    "    je crée la colonne 'argent' grâce à la fonction 'argent_posible'\n",
    "    je crée la colonne 'telephone' grâce à la fonction 'telephone_posible'\n",
    "    je crée la colonne 'email' grâce à la fonction 'email_posible'\n",
    "    je crée la colonne 'lien' grâce à la fonction 'lien_posible'\n",
    "    je crée la colonne 'maj' grâce à la fonction 'mot_maj_posible'\n",
    "    je crée la colonne 'long' grâce à la fonction 'long_posible'\n",
    "    \"\"\"    \n",
    "    df['mot_cles'] = df['sms'].apply(mot_cle_posible)\n",
    "    df['argent'] = df['sms'].apply(argent_posible)\n",
    "    df['telephone'] = df['sms'].apply(telephone_posible)\n",
    "    df['email'] = df['sms'].apply(email_posible)\n",
    "    df['lien'] = df['sms'].apply(lien_posible)\n",
    "    df['maj'] = df['sms'].apply(mot_maj_posible)\n",
    "    df['long'] = df['sms'].apply(long_posible)\n",
    "    df['mot'] = df['sms'].apply(nb_mot_posible)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la fonction qui fait le pré-processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_csv (df_new) :\n",
    "    \"\"\"\n",
    "    -------------------------\n",
    "    \"\"\"\n",
    "    # prépare les données x et y pour le train_test_split\n",
    "    df_new_papel = taitement_na_duplic(df_new)\n",
    "    y_papel = df_new_papel['classification']\n",
    "    x_papel = df_new_papel['sms']\n",
    "\n",
    "    x_papel_df = x_papel.to_frame()\n",
    "    x_papel_vect = vectorisation_df(x_papel_df)\n",
    "\n",
    "    # x et y utilisé pour le train_test_split\n",
    "    x_new = x_papel_vect.drop('sms', axis=1)\n",
    "    y_new = LabelEncoder().fit_transform(y_papel)\n",
    "\n",
    "    # résultat de la pipeline\n",
    "    y_pred = model_pip.predict( x_new )\n",
    "    model_pip.score( x_new, y_new )\n",
    "\n",
    "    score = accuracy_score(y_new, y_pred)\n",
    "    confusion_matrix = confusion_matrix(y_new, y_pred)\n",
    "\n",
    "    N, train_score, val_score = learning_curve(model_pip, x_train, y_train, scoring='f1',\n",
    "                                            train_sizes=np.linspace(0.1, 1, 10))\n",
    "\n",
    "    return x_new_test_test, y_new_test_test, y_pred, score, confusion_matrix, N, train_score, val_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_IA ( df_init, model_parametre_init):\n",
    "\n",
    "    # enregistre les parametres d'entrée comme valeur de classe\n",
    "    df= df_init\n",
    "    model_parametre = model_parametre_init\n",
    "\n",
    "    # prépare les données x et y pour le train_test_split\n",
    "    df_papel = taitement_na_duplic(df)\n",
    "    y_papel = df_papel['classification']\n",
    "    x_papel = df_papel['sms']\n",
    "    x_papel_df = x_papel.to_frame()\n",
    "    x_papel_vect = vectorisation_df(x_papel_df)\n",
    "    \n",
    "    # x et y utilisé pour le train_test_split\n",
    "    x_papel_vect_2 = x_papel_vect.drop('sms', axis=1)\n",
    "    y_papel_encoder = LabelEncoder().fit_transform(y_papel)\n",
    "    \n",
    "    # train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split ( x_papel_vect_2, y_papel_encoder, train_size = 0.80, test_size = 0.20, random_state = 123 )\n",
    "\n",
    "    # Catégorise les colonnes de x pour le make_column_transformer\n",
    "    norm_num = ['long' , 'mot']\n",
    "    bool_one_hot = ['mot_cles','argent','telephone','email','lien','maj']\n",
    "    \n",
    "    # pipeline de transformation\n",
    "    one_hot_encoder_pip = make_pipeline ( OneHotEncoder() )\n",
    "    min_max_scaler_pip = make_pipeline ( MinMaxScaler () )\n",
    "    \n",
    "    # make_column_transformer\n",
    "    transform_colonne = make_column_transformer (( one_hot_encoder_pip, bool_one_hot ), \n",
    "                                                    ( min_max_scaler_pip, norm_num ))\n",
    "    \n",
    "    # pipeline principale\n",
    "    model_pip = make_pipeline(transform_colonne, model_parametre)\n",
    "\n",
    "    # entrainement de la pipeline\n",
    "    model_pip.fit( x_train, y_train )\n",
    "\n",
    "    # résultat de la pipeline\n",
    "    y_pred = model_pip.predict( x_test )\n",
    "    model_pip.score( x_test, y_test )\n",
    "\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    \"\"\"\n",
    "    confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    N, train_score, val_score = learning_curve(model_pip, x_train, y_train, scoring='f1',\n",
    "                                            train_sizes=np.linspace(0.1, 1, 10))\n",
    "    \"\"\"\n",
    "    return x_train, y_train, x_test, y_test, y_pred, score, model_pip#, confusion_matrix, N, train_score, val_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File C:/Users/sandy/Documents/devIA/brief/SPAM/rendu/model_parametre.json does not exist",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[14], line 6\u001b[0m\n    df_json = pd.read_json('C:/Users/sandy/Documents/devIA/brief/SPAM/rendu/model_parametre.json', encoding = \"utf-8\", dtype=False)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:760\u001b[0m in \u001b[0;35mread_json\u001b[0m\n    json_reader = JsonReader(\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:861\u001b[0m in \u001b[0;35m__init__\u001b[0m\n    data = self._get_data_from_filepath(filepath_or_buffer)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:917\u001b[1;36m in \u001b[1;35m_get_data_from_filepath\u001b[1;36m\n\u001b[1;33m    raise FileNotFoundError(f\"File {filepath_or_buffer} does not exist\")\u001b[1;36m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m\u001b[1;31m:\u001b[0m File C:/Users/sandy/Documents/devIA/brief/SPAM/rendu/model_parametre.json does not exist\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# ouverture des fichiers\n",
    "####\n",
    "\n",
    "# ouverture du fichier model_parametre.json\n",
    "df_json = pd.read_json('C:/Users/sandy/Documents/devIA/brief/SPAM/rendu/model_parametre.json', encoding = \"utf-8\", dtype=False)\n",
    "print(\"df = \", df_json)\n",
    "\n",
    "# ouverture du fichier SMSSpamCollection\n",
    "df_spam = pd.read_csv('https://raw.githubusercontent.com/remijul/dataset/master/SMSSpamCollection', \n",
    "                 sep='\\t',on_bad_lines='skip', header=None)\n",
    "\n",
    "# ouverture du fichier best_model.csv\n",
    "df_best_model = pd.read_csv('C:/Users/sandy/Documents/devIA/brief/SPAM/rendu/best_model.csv', sep=',',on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CategoricalNB()\n",
    "x_train, y_train, x_test, y_test, y_pred, score, model_pip = model_IA(df_spam, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, train_score, val_score = learning_curve(model_pip, x_train, y_train, scoring='f1',\n",
    "                                            train_sizes=np.linspace(0.1, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "choix_model = 'SVC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_choix = []\n",
    "params_str = []\n",
    "df_choix = df_json.loc[df_json['model']==choix_model]\n",
    "params_str = df_choix.iloc[0][\"param_grid\"]\n",
    "\n",
    "params = {}\n",
    "for element in params_str :\n",
    "    value_type = params_str[element].split(\",\")\n",
    "    \n",
    "    value_type_type = []\n",
    "    for i in value_type :\n",
    "        print(\"i =\", i)\n",
    "        if i == 'None' :\n",
    "            i_type = None\n",
    "        if i == 'True' :\n",
    "            i_type = True\n",
    "        if i == 'False' :\n",
    "            i_type = False\n",
    "        if i != 'None' and i != 'True' and i != 'False' :\n",
    "            try :\n",
    "                i_type = int(i)\n",
    "            except :\n",
    "                try :\n",
    "                    i_type = float(i)\n",
    "                except :\n",
    "                    i_type = str(i)\n",
    "                        \n",
    "        print('i_type =',type(i_type))\n",
    "        value_type_type.append(i_type)\n",
    "        \n",
    "    print(\"element =\",element,\"element_type = \", type(element))\n",
    "    print(\"value =\",value_type_type,\"value_type = \", type(value_type_type))\n",
    "    \n",
    "    #params_str[element].append(element_type)\n",
    "    \n",
    "    params[element] = value_type_type\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid = GridSearchCV(model, param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "model_grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Parameters = model_grid.best_params_\n",
    "Best_Accuracy = model_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teste avec un nouveau sms :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detetion_de_spam(sms) :\n",
    "    \n",
    "    x = np.array([sms]).reshape(1, 1)\n",
    "\n",
    "    # prépare les données x et y pour le train_test_split\n",
    "    x_papel = x\n",
    "\n",
    "    x_papel_df = pd.DataFrame(x_papel, columns=['sms'])\n",
    "    x_papel_vect = vectorisation_df(x_papel_df)\n",
    "\n",
    "    # x et y utilisé pour le train_test_split\n",
    "    x_new = x_papel_vect.drop('sms', axis=1)\n",
    "\n",
    "    # résultat de la pipeline\n",
    "    y_pred = model_pip.predict( x_new )\n",
    "    y_pred_proba = model_pip.predict_proba( x_new )\n",
    "\n",
    "    return y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teste 1 : ham = \"Thanx 4 2day! U r a goodmate I THINK UR RITE SARY! ASUSUAL!1 U CHEERED ME UP! LOVE U FRANYxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_pred_proba = detetion_de_spam(\"Thanx 4 2day! U r a goodmate I THINK UR RITE SARY! ASUSUAL!1 U CHEERED ME UP! LOVE U FRANYxxxxx\")\n",
    "pourcent_ham = round( (y_pred_proba[0][0]*100) , 2)\n",
    "pourcent_spam = round( (y_pred_proba[0][1]*100) , 2)\n",
    "\n",
    "if y_pred == 0 :\n",
    "    print(f\"ham détecté avec {pourcent_ham} % de chance d'être vrai\")\n",
    "else :\n",
    "    print(f\"spam détecté avec {pourcent_spam} % de chance d'être vrai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teste 2 : spam = \"Refused a loan? Secured or Unsecured? Can't get credit? Call free now 0800 195 6669 or text back 'help' & we will!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_pred_proba = detetion_de_spam(\"Refused a loan? Secured or Unsecured? Can't get credit? Call free now 0800 195 6669 or text back 'help' & we will!\")\n",
    "pourcent_ham = round( (y_pred_proba[0][0]*100) , 2)\n",
    "pourcent_spam = round( (y_pred_proba[0][1]*100) , 2)\n",
    "\n",
    "if y_pred == 0 :\n",
    "    print(f\"ham détecté avec {pourcent_ham} % de chance d'être vrai\")\n",
    "else :\n",
    "    print(f\"spam détecté avec {pourcent_spam} % de chance d'être vrai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
